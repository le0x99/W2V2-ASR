{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# initial point\n",
    "p1 = np.array([-0.83483301, -0.16904167, 0.52390721])\n",
    "\n",
    "# compare with those vectors:\n",
    "P = np.array([\n",
    "    # similar point\n",
    " [-0.83455951, -0.16862266, 0.52447767],\n",
    "    # distractors\n",
    " [ 0.70374682, -0.18682394, -0.68544673],\n",
    " [ 0.15465702,  0.32303224,  0.93366556],\n",
    " [ 0.53043332, -0.83523217, -0.14500935],\n",
    " [ 0.68285685, -0.73054075,  0.00409143],\n",
    " [ 0.76652431,  0.61500886,  0.18494479]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure how similar two vectors are to each other, we need a way of measuring distance. In 2 or 3 dimensions the euclidian distance (“ordinary” or straight-line distance) is a great choice for measuring the distance between two points. However, in a large dimensional space, all points tend to be far apart by the euclidian measure. In higher dimensions, the angle between vectors is a more effective measure. The cosine distance measures the cosine of the angle between the vectors. The cosine of identical vectors is 1 while orthogonal and opposite vectors are 0 and -1 respectively. More similar vectors will result in a larger number. Calculating the cosine distance is done by taking the dot product of the vectors.\n",
    "\n",
    "- cos_sim = $ x_i @ x_j$\n",
    "- 1 if $x_i = x_j$\n",
    "- 0 if orthog.($x_i, x_j$)\n",
    "- -1 if oppos.($x_i, x_j$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 28.1 µs\n"
     ]
    }
   ],
   "source": [
    "# collect the distances\n",
    "%time\n",
    "sims = np.array([p1@p_j for p_j in P])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "# Better : use linear algebra instead of for loop\n",
    "%time\n",
    "sims = p1@P.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p1@P.T)[0] == p1.T@P[0] == p1@P[0] == p1@P[0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax the distances\n",
    "\n",
    "When calculating the loss for categorical cross-entropy, the first step is to take the softmax of the values, then the negative log of the labeled category.\n",
    "\n",
    "$$\n",
    "s(x_i) = \\frac{exp(x_i)}{\\sum_{j} exp(x_j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = np.exp(sims) / np.exp(sims).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42967915, 0.06330716, 0.21453534, 0.10835722, 0.10135234,\n",
       "       0.08276879])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "Given $C$ categories, first obtain the softmax :\n",
    "\n",
    "$$\n",
    "s(x_i) = \\frac{exp(x_i)}{\\sum_{j}^{C} exp(x_j)}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "C E=-\\sum_{i}^{C} t_{i} \\log \\left(s(x_{i})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss\n",
    "\n",
    "\n",
    "Contrastive loss looks suspiciously like the softmax function. That’s because it is, with the addition of the vector similarity and a temperature normalization factor. The similarity function is just the cosine distance that we talked about before. The other difference is that values in the denominator are the cosign distance from the positive example to the negative samples. Not very different from CrossEntropyLoss. The intuition here is that we want our similar vectors to be as close to 1 as possible, since -log(1) = 0, that’s the optimal loss. We want the negative examples to be close to 0 , since any non-zero values will reduce the value of similar vectors.\n",
    "\n",
    "$$\n",
    "\\ell_{i, j}=-\\log \\frac{\\exp \\left(\\operatorname{sim}\\left(\\boldsymbol{z}_{i}, \\boldsymbol{z}_{j}\\right) / \\tau\\right)}{\\sum_{k=1}^{2 N}{ }_{[k \\neq i]} \\exp \\left(\\operatorname{sim}\\left(\\boldsymbol{z}_{i}, \\boldsymbol{z}_{k}\\right) / \\tau\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = sims/temp\n",
    "logits = np.exp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = - np.log(logits[0] / logits.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9068650660314756e-05"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
